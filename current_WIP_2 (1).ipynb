{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99152a5-9589-478e-b9d8-8ca00930a263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (25.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.45.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tiktoken in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --only-binary=:all: tiktoken\n",
    "%pip install --upgrade pip setuptools wheel\n",
    "%pip install tiktoken --only-binary=:all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2edc27c-7736-4342-b4e0-10bfa48f1700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install -qU \\\n",
    "    langchain==0.3.* \\\n",
    "    langchain_openai==0.3.* \\\n",
    "    langchain_community \\\n",
    "    unstructured[md]==0.17.* \\\n",
    "    langgraph==0.4.* \\\n",
    "    websockets==15.0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9643295-8a25-49fe-bcfb-b9405d59a850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31733/1797179756.py:22: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "from langchain_community.utilities.requests import RequestsWrapper\n",
    "from langchain_community.agent_toolkits.openapi import planner\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.agent_toolkits.openapi.spec import reduce_openapi_spec\n",
    "import requests\n",
    "\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableMap, RunnableAssign\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pp\n",
    "from IPython.display import Markdown as render_md\n",
    "import pynvml  # type: ignore[import]\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea17aa6-08cf-44b9-9a93-6384d3d0046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "# openai api need to be set up before runnung the below cell. \n",
    "\n",
    "import getpass # for testing. remove for compilation \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebb211ae-dff1-4e42-8f56-eaa2ecacde1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supporting_documents():\n",
    "    current = os.getcwd()\n",
    "\n",
    "    while True:\n",
    "        candidate = os.path.join(current, 'GAI-3101-CAP')\n",
    "        if os.path.isdir(candidate):\n",
    "            target = os.path.join(candidate, 'capstone/support-info')\n",
    "            return os.path.relpath(target, os.getcwd())\n",
    "\n",
    "        parent = os.path.dirname(current)\n",
    "        if parent == current:\n",
    "            break  # Reached root without finding 'GAI-3101-CAP'\n",
    "        current = parent\n",
    "\n",
    "    raise FileNotFoundError(\"Could not find a directory containing 'GAI-3101-CAP'\")\n",
    "\n",
    "SUPPORT_DIR = get_supporting_documents()\n",
    "\n",
    "\n",
    "def get_files_with_extensions(folder_path, extensions):\n",
    "    matched_files = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext.lower()) for ext in extensions):\n",
    "                matched_files.append(os.path.join(root, file))\n",
    "    return matched_files\n",
    "\n",
    "get_files_with_extensions(SUPPORT_DIR, ['.md'])\n",
    "\n",
    "md_paths = get_files_with_extensions(SUPPORT_DIR,['.md'])\n",
    "md_docs = []\n",
    "for path in md_paths:\n",
    "    text_loader = TextLoader(path)\n",
    "    doc = text_loader.load()\n",
    "    md_docs += doc\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "vector_store = InMemoryVectorStore(embedding_model)\n",
    "vector_store.add_documents(documents=md_docs)\n",
    "\n",
    "\n",
    "doc_retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", #could also do MMR if we want to avoid repeated/very similar docs\n",
    "    search_kwargs={\n",
    "        'k': 3 # number of documents to consider\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5314d212-b0af-4c96-887f-c00f8e4220f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = '''\n",
    "You are a helpful AI bot being used to assist Beanbotics., a company that sell automated robotic arms that make coffee.\n",
    "Your job is to automate the initial processing of\n",
    "support tickets. You will read incoming tickets, extract key information,\n",
    "classify the requests into categories, assign priority levels, and update the ticketing\n",
    "system with this information\n",
    "\n",
    "Instructions:\n",
    "- Use only the content found in the provided context documents unless explicitly asked to use outside information\n",
    "- Include inline citations using the `source` field from metadata (e.g., [source]).\n",
    "- If documents conflict, describe the differing perspectives and cite each source (e.g., [source A], [source B]).\n",
    "- If the answer cannot be found, say: “The context does not contain that information.”\n",
    "- If the question is ambiguous, explain your interpretation before answering.'\n",
    "'''\n",
    "\n",
    "human_msg = \"\"\"\n",
    "Please answer this query from the user\n",
    "<original user query>\n",
    "{query}\n",
    "</original user query>\n",
    "\n",
    "---\n",
    "Use these docs to answer the original user query\n",
    "<context documents>\n",
    "{context_docs}\n",
    "</context documents>\n",
    "\"\"\"\n",
    "\n",
    "generation_template = ChatPromptTemplate([\n",
    "    (\"system\", system_msg),\n",
    "    (\"human\", human_msg),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "107be360-d0ac-4ba6-8c05-8e24ce375a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket: 1a2b3c4d-0002-0000-0000-000000000002\n",
      "Ticket: 1a2b3c4d-0004-0000-0000-000000000004\n",
      "Ticket: 843628c5-80fe-488b-afd1-12877ad0f29c\n",
      "Ticket: 4cf79b5d-3512-4da7-bc3a-1eb5040733d6\n",
      "Ticket: 61f44ec5-b77e-4069-8906-5cd09023d077\n",
      "Ticket: ab4cdd2f-6cad-4e3a-af5e-4df6a272c15d\n",
      "Ticket: 54460e4f-e1b7-41a8-9cf6-dd89da206831\n",
      "Ticket: 48b5ff31-7747-4d22-bef8-2d6639af5730\n",
      "Ticket: ee5942d3-a573-4470-87aa-b9b8820ec03e\n",
      "Ticket: 73ceba04-1d3d-46aa-8e30-3be27bc1e043\n",
      "Ticket: 28f4575f-2ecd-4f2b-9358-552a43fb3594\n",
      "Ticket: 1f86d6c3-13f5-413e-b8ad-f78538ca5979\n",
      "Ticket: 7d5f70db-abca-4a4a-b15c-5dd0d6dac326\n",
      "Ticket: ced115c8-e853-4f50-b802-15b255b7e78d\n",
      "Ticket: 1fe9ed6d-c5c9-4b38-8fd6-def8ab195db5\n",
      "Ticket: 47a7720d-1821-4e27-9f2e-25e0b8daa8a4\n",
      "Ticket: ecffca7c-b210-43e1-9d0d-4872d9044de7\n",
      "\n",
      "------------------- Starting Connection and Agent -------------------\n",
      "WebSocket connection established.\n",
      "WebSocket connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Load the OpenAPI specification from the running ticketing system\n",
    "root = \"http://localhost:3000\"\n",
    "api_spec_url = f\"{root}/api/docs/openapi.json\"\n",
    "\n",
    "# Download and parse the OpenAPI spec\n",
    "response = requests.get(api_spec_url)\n",
    "data = response.json()\n",
    "data['servers'] = [{'url': root}]\n",
    "openapi_spec = reduce_openapi_spec(data, dereference=False)\n",
    "\n",
    "### Try to get it to categorize all tickets\n",
    "requests_wrapper = RequestsWrapper()\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.0)\n",
    "\n",
    "import requests\n",
    "\n",
    "agent = planner.create_openapi_agent(\n",
    "    api_spec=openapi_spec,\n",
    "    requests_wrapper=requests_wrapper,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    handle_parsing_errors=True,\n",
    "    allow_operations=['GET', 'POST', 'PUT', 'PATCH', 'DELETE']\n",
    ")\n",
    "\n",
    "\n",
    "response = requests.get(root+ \"/api/tickets\")\n",
    "tickets = response.json()\n",
    "\n",
    "for t in tickets:\n",
    "    ticket_id = t[\"id\"]\n",
    "    ticket_cat = t[\"category\"]\n",
    "    print(\"Ticket:\", ticket_id)\n",
    "\n",
    "import json\n",
    "import websockets\n",
    "\n",
    "WS_URL = \"ws://localhost:3000/ws\"\n",
    "# This async function connects to the WebSocket and listens for ticket updates\n",
    "# Once a ticket update is received, it yields it for processing.\n",
    "async def listen_for_ticket_updates():\n",
    "    print(\"\\n------------------- Starting Connection and Agent -------------------\")\n",
    "    # Establish a connection to the WebSocket server\n",
    "    async with websockets.connect(WS_URL) as websocket:\n",
    "        print(\"WebSocket connection established.\")\n",
    "        try:\n",
    "            # Keep listening for messages from the server\n",
    "            while True:\n",
    "                message = await websocket.recv()  # Wait for a new message\n",
    "                yield json.loads(message)\n",
    "        except websockets.ConnectionClosed:\n",
    "            print(\"WebSocket connection closed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"WebSocket error: {e}\")\n",
    "\n",
    "import asyncio\n",
    "\n",
    "# placing the chain here so the RAG system can influence instructions \n",
    "rag_chain = (\n",
    "    RunnableMap(\n",
    "        query=RunnablePassthrough(),\n",
    "        context_docs=doc_retriever\n",
    "    )\n",
    "    | RunnableMap(\n",
    "        context_docs=RunnableLambda(lambda x: x['context_docs']),\n",
    "        llm_response=(\n",
    "            generation_template\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# --- Async wrapper for the agent ---\n",
    "async def run_agent_async(instructions: str):\n",
    "    \"\"\"Run the agent in a non-blocking way.\"\"\"\n",
    "    # agent.invoke is synchronous, so run it in a thread\n",
    "    return await asyncio.to_thread(agent.invoke, instructions)\n",
    "\n",
    "# --- Async agent helper ---\n",
    "async def run_agent(ticket_id, action_description, instructions):\n",
    "    \"\"\"Run an agent action asynchronously with logging.\"\"\"\n",
    "    print(f'{action_description} for ticket: {ticket_id}')\n",
    "    return await run_agent_async(instructions.strip())\n",
    "\n",
    "\n",
    "# --- Simplified async ticket functions ---\n",
    "async def determine_category(ticket_id):\n",
    "    await run_agent(\n",
    "        ticket_id,\n",
    "        \"Categorizing ticket\",\n",
    "        f\"\"\"\n",
    "Assign this ticket {ticket_id} one category from: Mechanical, Quality, Maintenance, Technical, Awaiting Details.\n",
    "Use “Awaiting Details” ONLY if insufficient info exists. Otherwise, POST the assigned category.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "async def determine_priority(ticket_id):\n",
    "    await run_agent(\n",
    "        ticket_id,\n",
    "        \"Determining ticket priority\",\n",
    "        f\"\"\"\n",
    "Assign this ticket {ticket_id} a priority: High, Medium, or Low. \n",
    "If the category is “Awaiting Details,” set priority to Low. Otherwise, POST the assigned priority.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "async def determine_response(ticket_id, instructions):\n",
    "    rag_result = rag_chain.invoke({\"query\": instructions})\n",
    "    enriched_instructions = f\"\"\"\n",
    "Use the following documentation context to complete the task:\n",
    "\n",
    "{rag_result['llm_response']}\n",
    "\n",
    "Original instructions:\n",
    "{instructions}\n",
    "\"\"\"\n",
    "    await run_agent(\n",
    "        ticket_id,\n",
    "        \"Checking for automatic response\",\n",
    "        f\"\"\"\n",
    "    GET ticket {ticket_id}. Inform your response based off the provided documents {enriched_instructions} \n",
    "If category is Awaiting Details, POST a response requesting more info; otherwise do nothing.\n",
    "Use JSON: ticket_id={ticket_id}, author=\"Support Agent\", message=\"<friendly message>\"\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "async def determine_status(ticket_id):\n",
    "    await run_agent(\n",
    "        ticket_id,\n",
    "        \"Checking ticket status\",\n",
    "        f\"\"\"\n",
    "Check ticket {ticket_id}. \n",
    "If priority is High and ticket seems urgent, update status to \"escalated\". Otherwise, do nothing.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "async def determine_escalation(ticket_id):\n",
    "    await run_agent(\n",
    "        ticket_id,\n",
    "        \"Checking if escalation is required\",\n",
    "        f\"\"\"\n",
    "Check ticket {ticket_id}. \n",
    "If status is escalated, POST an automatic escalation response: \n",
    "ticket_id={ticket_id}, author=\"Support Agent\", message=\"<context-aware escalation message>\"\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "async def auto_respond(ticket_id):\n",
    "    await run_agent(\n",
    "        ticket_id,\n",
    "        \"Checking if auto-response is needed\",\n",
    "        f\"\"\"\n",
    "GET ticket {ticket_id}'s most recent response. \n",
    "If last response is from Support Agent, ignore. Otherwise, POST a placeholder response using the provided documentation:\n",
    "ticket_id={ticket_id}, author=\"Support Agent\", message=\"<friendly placeholder>\"\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Async listener loop ---\n",
    "async for message in listen_for_ticket_updates():\n",
    "    ticket_id = message.get('ticketId')\n",
    "    update_type = message.get('updateType')\n",
    "    print(\"----------------- MESSAGE -------------\")\n",
    "    print(message)\n",
    "    print(\"----------------- Ticket ID -------------\")\n",
    "    print(ticket_id)\n",
    "    print(\"----------------- UPDATE TYPE -------------\")\n",
    "    print(update_type)\n",
    "\n",
    "    if update_type == 'created':\n",
    "        print(\"CREATED\")\n",
    "        # Run all ticket handlers asynchronously in sequence\n",
    "        await determine_category(ticket_id)\n",
    "        await determine_priority(ticket_id)\n",
    "        await determine_response(ticket_id, instructions)\n",
    "        await determine_status(ticket_id)\n",
    "        await determine_escalation(ticket_id)\n",
    "\n",
    "    elif update_type == 'response':\n",
    "        await auto_respond(ticket_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c1b03-8fd6-4472-8fc9-62d31cb0bc67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
